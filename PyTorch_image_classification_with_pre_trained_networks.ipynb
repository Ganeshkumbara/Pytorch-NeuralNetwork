{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDRQLJirbFxj"
   },
   "source": [
    "## Pytorch to classify input images using the following State-of-the-art classification networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zAqnt5IaYaG"
   },
   "source": [
    "##Classification Networks\n",
    "* VGG16\n",
    "* VGG19\n",
    "* Inception\n",
    "* DenseNet\n",
    "* ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7x5buIq2bHy2",
    "outputId": "de76e33d-d9e9-4138-d5e3-04a636af540d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\gsubramani\\documents\\codebase\\pytorch-neuralnetwork\\.venv\\lib\\site-packages (from opencv-python) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3x6_UbgebP7g"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sqXWx_EmcptQ"
   },
   "outputs": [],
   "source": [
    "# specify image dimension\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# specify ImageNet mean and standard deviation\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# determine the device we will be using for inference\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# specify path to the ImageNet labels\n",
    "IN_LABELS = \"utils/ilsvrc2012_wordnet_lemmas.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J9KW3Ln3czyd"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gNj4dJ07muzN"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "\t# swap the color channels from BGR to RGB, resize it, and scale\n",
    "\t# the pixel values to [0, 1] range\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\timage = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\timage = image.astype(\"float32\") / 255.0\n",
    "\t# subtract ImageNet mean, divide by ImageNet standard deviation,\n",
    "\t# set \"channels first\" ordering, and add a batch dimension\n",
    "\timage -= MEAN\n",
    "\timage /= STD\n",
    "\timage = np.transpose(image, (2, 0, 1))\n",
    "\timage = np.expand_dims(image, 0)\n",
    "\t# return the preprocessed image\n",
    "\treturn image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-sIxwCum5pr",
    "outputId": "145a3148-3fc5-4619-eb61-1e92876aef63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsubramani\\Documents\\Codebase\\Pytorch-NeuralNetwork\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gsubramani\\Documents\\Codebase\\Pytorch-NeuralNetwork\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\gsubramani\\Documents\\Codebase\\Pytorch-NeuralNetwork\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\gsubramani\\Documents\\Codebase\\Pytorch-NeuralNetwork\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\gsubramani\\Documents\\Codebase\\Pytorch-NeuralNetwork\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\gsubramani\\Documents\\Codebase\\Pytorch-NeuralNetwork\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading vgg19...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = \"vgg19\"\n",
    "\n",
    "# define a dictionary that maps model names to their classes\n",
    "# inside torchvision\n",
    "MODELS = {\n",
    "\t\"vgg16\": models.vgg16(pretrained=True),\n",
    "\t\"vgg19\": models.vgg19(pretrained=True),\n",
    "\t\"inception\": models.inception_v3(pretrained=True),\n",
    "\t\"densenet\": models.densenet121(pretrained=True),\n",
    "\t\"resnet\": models.resnet50(pretrained=True)\n",
    "}\n",
    "# load our the network weights from disk, flash it to the current\n",
    "# device, and set it to evaluation mode\n",
    "print(\"[INFO] loading {}...\".format(model_type))\n",
    "model = MODELS[model_type].to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgrXdPkinMLm",
    "outputId": "908d98f6-eb03-425f-d755-eb10f73f7d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading image...\n",
      "[INFO] loading ImageNet labels...\n"
     ]
    }
   ],
   "source": [
    "image_path = \"utils/papillon.jpg\"\n",
    "\n",
    "# load the image from disk, clone it (so we can draw on it later),\n",
    "# and preprocess it\n",
    "print(\"[INFO] loading image...\")\n",
    "image = cv2.imread(image_path)\n",
    "orig = image.copy()\n",
    "image = preprocess_image(image)\n",
    "# convert the preprocessed image to a torch tensor and flash it to\n",
    "# the current device\n",
    "image = torch.from_numpy(image)\n",
    "image = image.to(DEVICE)\n",
    "# load the preprocessed the ImageNet labels\n",
    "print(\"[INFO] loading ImageNet labels...\")\n",
    "imagenetLabels = dict(enumerate(open(IN_LABELS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6ZQalNbqstx",
    "outputId": "00783668-6f50-458a-aef2-6870b5648e86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image with 'vgg19'...\n",
      "0. papillon: 99.98%\n",
      "1. Japanese_spaniel: 0.01%\n",
      "2. Chihuahua: 0.00%\n",
      "3. Yorkshire_terrier: 0.00%\n",
      "4. Pomeranian: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# classify the image and extract the predictions\n",
    "print(\"[INFO] classifying image with '{}'...\".format(model_type))\n",
    "logits = model(image)\n",
    "probabilities = torch.nn.Softmax(dim=-1)(logits)\n",
    "sortedProba = torch.argsort(probabilities, dim=-1, descending=True)\n",
    "# loop over the predictions and display the rank-5 predictions and\n",
    "# corresponding probabilities to our terminal\n",
    "for (i, idx) in enumerate(sortedProba[0, :5]):\n",
    "\tprint(\"{}. {}: {:.2f}%\".format\n",
    "\t\t(i, imagenetLabels[idx.item()].strip(),\n",
    "\t\tprobabilities[0, idx.item()] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "hQnY1kW7qrJo",
    "outputId": "f443834a-ca1e-4a11-e23a-9e12eaa1d2e1"
   },
   "outputs": [],
   "source": [
    "# from google.colab.patches import cv2_imshow\n",
    "# draw the top prediction on the image and display the image to\n",
    "# our screen\n",
    "(label, prob) = (imagenetLabels[probabilities.argmax().item()],\n",
    "\tprobabilities.max().item())\n",
    "cv2.putText(orig, \"Label: {}, {:.2f}%\".format(label.strip(), prob * 100),\n",
    "\t(10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "cv2.imshow(\"test\",orig)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYVtk10Ltrvy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
